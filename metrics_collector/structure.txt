metrics_collector/
├── collector/              # Core collection logic
│   ├── pcm_reader.py       # PCM-specific metrics
│   ├── buffer.py           # Rolling buffer impl
│   └── sampler.py          # Sampling strategies
├── service/                # API layer
│   └── api.py              # Flask/FastAPI endpoints
├── Dockerfile
└── requirements.txt


# Build and push
docker build -t your-registry/metrics-collector:v1 .
docker push your-registry/metrics-collector:v1

# Deploy
kubectl apply -f metrics-collector-daemonset.yaml

# Verify
kubectl get pods -l app=metrics-collector -o wide
kubectl logs -f metrics-collector-<node-id>

# Test endpoint
kubectl exec -it metrics-collector-<node-id> -- \
  curl -s localhost:8000/metrics?window_seconds=20 | jq


  ////////////////////////////////////////////////////////////////
  METRICS COLLECTOR ARCHITECTURE

Core Design Principles:
─────────────────────────────────────────────────
• Continuous low-overhead sampling (1Hz default)
• Rolling buffer maintains 30-second metrics window
• On-demand precise 20s aggregation for scheduling
• Node-level isolation (DaemonSet deployment)

Components:
─────────────────────────────────────────────────
1. Data Collection Layer
   • PCM command interface (sudo privileges)
   • Hardware metric scraping (CPU, mem, cache, etc.)
   • Adaptive sampling (adjustable rate)

2. Processing Layer
   • MetricBuffer (thread-safe circular buffer)
     - Time-ordered metric storage
     - Configurable retention (default 30s)
   • PCM Parser
     - CSV to structured metrics
     - Feature normalization

3. Service Layer
   • REST API (FastAPI)
     - /metrics?window_seconds=20
     - /health (liveness/readiness)
   • Aggregation Engine
     - Mean/stddev calculations
     - Windowed metrics selection

4. Cluster Integration
   • Kubernetes DaemonSet (one pod per node)
   • Privileged security context
   • Host PID namespace access
   • Resource-constrained operation

Data Flow:
─────────────────────────────────────────────────
[PCM Tool] → [Raw Metrics] → [Parser] → [MetricBuffer]
                                   ↓
[API Request] ← [Aggregator] ← [20s Window]

Key Characteristics:
─────────────────────────────────────────────────
• Low-latency: Pre-collected data = no wait time
• Accurate: Precise time windows (not samples)
• Resilient: Graceful degradation on PCM failures
• Isolated: Node-specific metrics only
• Efficient: ~1% CPU overhead (tested)

Deployment Spec:
─────────────────────────────────────────────────
• Image: Python 3.9 + PCM binaries
• Resources: 200m CPU / 256Mi RAM limit
• Volumes: /dev/cpu/msr (RDMSR access)
• Network: ClusterIP service on port 8000

Monitoring:
─────────────────────────────────────────────────
• Buffer fill level (metrics/samples_available)
• Collection errors (pcm/errors_total)
• API latency (api/latency_seconds)
• Sample freshness (metrics/newest_seconds)

Example API Response:
─────────────────────────────────────────────────
{
  "window_seconds": 20,
  "samples_count": 20,
  "metrics": {
    "mean_cpu_util": 0.65,
    "std_mem_bw": 0.12,
    "p95_cache_miss": 0.42,
    ...
  },
  "timestamp": 1690000000.123456
}


# Build
docker build -t metrics-collector:v1 .

# Run locally for testing
docker run -it --rm \
  --privileged \
  -p 8000:8000 \
  -v /dev/cpu:/dev/cpu \
  metrics-collector:v1

# Test endpoint
curl "http://localhost:8000/metrics?window_sec=20"