===============================
SCHEDULER EXTENSION OPTIONS
===============================

1. PERIODIC REBALANCING / AUTO-REPACKING
----------------------------------------
Goal:
Continuously monitor deployed pods and reassign them to lower-risk nodes when interference or traffic causes SLO risk.

Concept:
- Periodically evaluate RiskScore = predicted_P99 / SLO_threshold for each node.
- If any node has RiskScore > 1, identify replicas that can be safely evicted.
- Move replicas to nodes with lower predicted risk.

Academic Value:
Mimics VM live migration but in a containerized context. Introduces continuous placement optimization beyond static schedulers.

Implementation:
- Run as a CronJob or controller.
- Use Kubernetes eviction API or `kubectl drain` logic.
- Combine with node tainting to prevent pods from re-landing on degraded nodes.


2. SMART SCALING (REPLICA COUNT OPTIMIZER)
------------------------------------------
Goal:
Proactively determine the minimum number of replicas needed to avoid SLO violations, using traffic forecasts and latency prediction.

Concept:
- Predict future RPS using ARIMA (or other methods).
- For each replica count (1 to N), estimate P99 using your ML predictor.
- Pick the smallest replica count where RiskScore < 1.

Academic Value:
Moves beyond CPU-based HPA. Combines prediction, interference awareness, and cost-efficient scaling into a proactive controller.

Implementation:
- Deploy as microservice or scheduled scaler.
- Integrate with K8s scaling API or create a custom controller.
- Use your trained predictor model as backend.


3. GLOBAL MULTI-REPLICA OPTIMIZATION
-------------------------------------
Goal:
Jointly optimize placement for a group of replicas, not one-by-one, to minimize cluster-wide SLO risk.

Concept:
- Simulate several candidate placement combinations.
- For each, compute aggregate RiskScore (e.g. sum or max).
- Deploy the replica set using the best configuration.

Academic Value:
Goes beyond greedy heuristics; introduces non-myopic, constraint-aware scheduling into Kubernetes research.

Implementation:
- Use beam search or greedy refinement to limit search space.
- Query predictor API for each simulated set.
- Bind pods manually or generate a scheduling plan.


4. INTERFERENCE INJECTION & RESILIENCE TESTING
----------------------------------------------
Goal:
Evaluate your schedulerâ€™s robustness by injecting controlled, reproducible interference patterns using iBench.

Concept:
- Deploy interference workloads (e.g. CPU/LLC/mem stressors) on schedule or on demand.
- Monitor how your scheduler detects and responds to performance degradation.

Academic Value:
Turns your testbed into a chaos-engineering environment for scheduling research. Allows measurement of detection time, action correctness, and recovery latency.

Implementation:
- Use YAML specs to define interference levels.
- Track recovery time and action path.
- Log metrics and state transitions for reproducibility.



// Create a Minikube Cluster with 2 nodes, each node should have 3 CPUs and 3072MB of memory. 
// Also I want to pin the CPU cores to each node
# Delete existing cluster
minikube delete --all

# Start control plane (pinned to CPU 6)
minikube start \
  --driver=docker \
  --cpus=3 \
  --memory=3072 \
  --container-runtime=containerd \
  --nodes=2 \
  --extra-config=kube-proxy.mode=ipvs

# Apply CPU pinning
docker update minikube --cpus=3 --cpuset-cpus="0-2"  
docker update minikube-m02 --cpus=3 --cpuset-cpus="3-5"

  --extra-config=kubelet.system-reserved=cpu=500m \
  --extra-config=kubelet.kube-reserved=cpu=500m


// Now I have cores 6-7 for external workloads
minikube ssh -n minikube
sudo systemctl stop kubelet
sudo rm /var/lib/kubelet/cpu_manager_state
sudo vi /var/lib/kubelet/config.yaml
  reservedSystemCPUs: "6-7" # maybe change more?
  cpuManagerPolicy: static
sudo systemctl restart kubelet

--extra-config=kube-proxy.mode=ipvs -> for round-robin

///////////////// Validation

1. Check Docker Container CPU Pinning
    # List Minikube containers and their CPU pinning
docker inspect minikube --format '{{.Name}} -> {{.HostConfig.CpusetCpus}}'
docker inspect minikube-m02 --format '{{.Name}} -> {{.HostConfig.CpusetCpus}}'


2. Check Inside the Nodes (via SSH)
minikube ssh -n minikube "cat /sys/fs/cgroup/cpuset.cpus"
minikube ssh -n minikube-m02 "cat /sys/fs/cgroup/cpuset.cpus"

3. Kubernetes-Level Validation
kubectl describe nodes | grep -A 5 "Allocatable"
