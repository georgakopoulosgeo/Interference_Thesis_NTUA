
===============================
Replica-Aware Slowdown Predictor
===============================

📌 Purpose:
Predict normalized tail-latency slowdown (norm_perf = baseline_P99 / observed_P99)
based on live Intel PCM hardware metrics, replica count, and RPS.

🎯 Inputs to the Model:
- Sliding-window statistics (mean, std, p95) from Intel PCM logs
  • Metrics include IPC, L2MISS, L3MISS, PhysIPC%, and C-state residencies
  • Computed per core (3, 4, 5) and also as averaged aggregates
- Given RPS (traffic load)
- Number of replicas already placed on the node

🎯 Output:
- Predicted normalized slowdown (norm_perf), e.g., 1.2 → 20% latency increase

🧠 Model Architecture:
- StandardScaler: normalizes all input features
- PolynomialFeatures (degree=2): captures interactions between metrics
- PCA: reduces dimensionality while retaining 95% variance
- RandomForestRegressor: robust nonlinear predictor

📁 Saved Model:
- Path: models/slowdown_predictor.pkl
- Format: scikit-learn Pipeline (joblib)

📊 Evaluation Summary:
- R² Score: High (≈ 0.95 on test set)
- MAE: Low (≈ 0.04)
- PCA retained variance: 95%

🔁 Training Dataset:
- Derived from 24 test scenarios across 4 replica levels, 9 RPS values, and multiple interference setups
- Each sample summarizes a 180s workload run via 36-row PCM log

🔄 Use Case in Scheduler:
At pod placement time:
  1. Collect live PCM metrics
  2. Add intended replica count and known/forecasted RPS
  3. Predict expected slowdown (norm_perf)
  4. Compute RiskScore = 1 / norm_perf
  5. Choose node with lowest RiskScore

===============================
🔧 Recommendations for Improvement
===============================
1. 🔍 Feature Selection:
   - Consider using mutual information or SHAP values to prune weak features
   - Try ablation testing to confirm which PCM metrics matter most

2. 🧪 Data Expansion:
   - Run more scenarios (especially for overlapping RPS/replica ranges)
   - Add controlled noise or jitter to simulate real-world metrics

3. 🌲 Model Alternatives:
   - Try XGBoost or LightGBM with early stopping
   - Consider a model ensemble (RandomForest + XGBoost + Ridge)

4. 🔁 Drift Detection:
   - Monitor real vs. predicted norm_perf over time in production
   - Trigger retraining when drift exceeds a threshold

5. ⚙️ Multi-Objective Extension:
   - Extend model output to include energy or cost if system metrics allow
   - Useful for joint optimization (latency vs. energy vs. placement density)

6. 📈 SHAP Analysis:
   - Use TreeExplainer for detailed feature attributions
   - Plot feature importances to drive system tuning



=======================================================================
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score

X = df_ml[feature_cols]
y = df_ml["norm_perf"]

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

model = XGBRegressor(
    n_estimators=200,
    max_depth=4,
    learning_rate=0.05,
    subsample=0.9,
    colsample_bytree=0.8,
    reg_lambda=1.0,
    random_state=42
)

model.fit(X_train, y_train, early_stopping_rounds=10, eval_set=[(X_test, y_test)], verbose=True)

y_pred = model.predict(X_test)
print("MAE:", mean_absolute_error(y_test, y_pred))
print("R²:", r2_score(y_test, y_pred))
